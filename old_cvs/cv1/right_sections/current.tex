% \cvsection{{\Large Currently Working On}}
% 	\cvachievement
% 	{\faBook}
% 	{Transformers at a Fraction (Microsoft)}
% 	{}
% 	\vspace{-7mm}
% 	\begin{justify}
% 		Engaged in a Microsoft Academic Partnership Grant project at NISER under Professor Subhankar Mishra, I am currently optimizing neural networks, specifically transformer layers. My goal is to reduce parameter count and computational overhead without compromising performance significantly. This project aims to make these models suitable for deployment on low-end devices, facilitating efficient predictions on mobile devices for instance.
% 		% More on it \href{https://www.niser.ac.in/~smishra/teach/cs460/23cs460/#aritraadhilproject}{here}.

% 		% \vspace{3mm}

% 		% \noindent GitHub Repository: \href{https://github.com/smlab-niser/quatLT23}{\textbf{smlab-niser/quatLT23}}
% 	\end{justify}




\cvsection{{\Large Currently Working On}}
\cvachievement
{\faBook}
{Applying Machine Learning and Monte Carlo on the Classical Ising Model}
{with Prof. Anamitra Mukherjee}
\vspace{-5mm}
\begin{justify}
    Implemented the Metropolis algorithm to solve the Ising model and study phase transitions in magnetic materials. With this data in hand, we can apply machine learning to determine various observables, bringing down computation costs.
\end{justify}



